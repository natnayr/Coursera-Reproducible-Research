Evidence-Based Data Analysis

Replication Summary
- Focus on validity of scientific claim
- Is this claim true?
- The ultimate standard for strengtening scientific evidence
- New investigators, data, analytical methods, labs, instruments
- Particularly important in studies that impact broad policy or regulatory decisions

------------------------------------------------------------------------

Reproducibility Summary
- Focus on validility of data analysis
- Can we trust this analysis?
- Argueably a minimum standard for scientific study
- New investigators, same data, same methods
- Important when replication is impossible for one-off data (e.g. polution index)

------------------------------------------------------------------------

Background and Underlying Treads

- Studies cannot be replicated, no time or money
- Technology is increasing data collection, BIG DATA
- Existing databases can be merged to become bigger databases
- Computing power allows more sophisticated analysis, even on "small" data
- For every field "X" there is a "Computational X"

------------------------------------------------------------------------

The Results?
- Basic analysis are difficult to describe
- Heavy computational requirements are made on people without training in
stats or computing
- Error are easily introduced
- Knowledge transfer is inhibited
- Results are difficult to replicate or reproduce
- Complicated analyses cannot be trusted.

------------------------------------------------------------------------

What is Reproducible Research?

In Replication:
- Author, Scientific Questions -> Protocol -> Nature -> Measure
Followed to Domain of Analytic Data -> Computational Results
	-> Figures, Tablets, Numerical Summaries
	-> Published Article -> Text

However in Reproducible Research, not all resources are not avalible:
- Measured Data -> Analytic Data -> Computational Results

- Transparency
- Data Avalibility
- Software / Methods Avalibility
- Improve Transfer of Knowledge

What we do not get:
- Validity / Correctness of analysis

What could go wrong?
- Reproducible but still wrong
- "can we trust this analysis??"
- Premise that the data/code avalible, people can check each other and the entire system is self-correcting
- Post-publication checking is important
- Assumes everyone plays by the same rules and wants to achieve the same goals (i.e. scientific discovery)

------------------------------------------------------------------------

Analogy
- Reproducibility lies in the area of downstream area

------------------------------------------------------------------------

Evidence-based Data Analysis
- Lots of tools and methods to apply
- Methods are standard enough to given field
- Apply throughly studied (via statistical research), mutually agreed upon methods to analyze data whenever possible
- Evidence to justify application of given methods.

------------------------------------------------------------------------

Why?
- Create analytic pipelines for evidence-based components - standardize it
- Deterministic Statistical Machine
- Evidence based analytic pipeline is established, should not mess with process
- Analysis with transparent box
- Reduce "researchers degrees of freedom"
- establishes pre-specified clinical trial protocol (in biostatistics field)


